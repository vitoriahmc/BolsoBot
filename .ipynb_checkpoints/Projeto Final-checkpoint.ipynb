{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = './camara'\n",
    "docs = []\n",
    "files = os.listdir(PATH)\n",
    "for f in files:\n",
    "    with open(PATH + '/' + f, 'r', encoding='utf-8') as file:\n",
    "        text = ' '.join(file.read().split('-')[2:])\n",
    "        docs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './gov'\n",
    "files = os.listdir(PATH)\n",
    "for f in files:\n",
    "    with open(PATH + '/' + f, 'r', encoding='utf-8') as file:\n",
    "        docs.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt', 'w', encoding='utf-8') as f:\n",
    "    text = ''.join(docs)\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E Brasil acima de tudo e Deus acima de todos.\n",
      "36942\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "processed_docs = []\n",
    "for doc in docs:\n",
    "    sents = nltk.sent_tokenize(doc)\n",
    "    for sent in sents:\n",
    "        if sent[0] not in ['(', ')'] and 'etc' not in sent.split(' '):\n",
    "            processed_docs.append(sent)\n",
    "print(processed_docs[-2])\n",
    "print(len(processed_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24766\n",
      "[' Obrigado pela oportunidade.', 'Eu estou muito feliz em retornar a esta Casa, rever velhos amigos e fazer novas amizades.(Palmas.)', 'Estamos aqui num dos centros do poder.', 'Juntos, Executivo, Legislativo e Judiciário têm um compromisso, como há pouco discursou aqui o nosso Presidente do Supremo Tribunal Federal, Ministro Toffoli.', 'A responsabilidade é de todos nós.', 'Pedimos a Deus que nos ilumine.', 'Agradeço por Ele ter salvado a minha vida há pouco tempo.', 'Quero dizer a todos: na topografia existem três nortes, o da quadrícula, o verdadeiro e o magnético, mas na democracia há só um norte: é o da nossa Constituição.(Palmas.)', 'Juntos, Presidente Toffoli, querido Rodrigo Maia e demais autoridades aqui da Mesa, vamos continuar, Presidente Temer, construindo o Brasil que o nosso povo merece.', 'Temos tudo, tudo, para sermos uma grande Nação.', 'A nossa união, que no momento estamos aqui ocupando cargos chave na República, pode, sim, mudar o destino desta grande Nação.', 'Acredito em Deus, acredito no povo brasileiro, acredito em nosso potencial.', 'Meu muito obrigado a todos.', 'Peço a Deus que nos ilumine a todos para continuarmos traçando os destinos que o nosso povo merece: a felicidade, o Brasil acima de tudo e Deus acima de todos.', 'Muito obrigado.']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "NUM_WORDS = 24766\n",
    "# NUM_WORDS = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token='<unknown>')\n",
    "tokenizer.fit_on_texts(processed_docs)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(len(tokenizer.word_index))\n",
    "print((processed_docs[0:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557204\n"
     ]
    }
   ],
   "source": [
    "max_sequence_len = 15\n",
    "\n",
    "sequences = []\n",
    "k=0\n",
    "for line in processed_docs:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "    for i in range(2, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        sequences.append(n_gram_sequence)\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "sequences = pad_sequences(sequences, maxlen=max_sequence_len+1, padding='pre')\n",
    "\n",
    "X = sequences[:, :-1]\n",
    "labels = sequences[:, -1]\n",
    "# y = to_categorical(labels, num_classes=NUM_WORDS)\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(557204, 15)\n",
      "(557204,)\n",
      "[ 380   33  634   13 3891    5  159   66 3323 1837]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0   85\n",
      "    69]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0   19\n",
      "   170]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0   19  170\n",
      "    33]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0   19  170   33\n",
      "   634]\n",
      " [   0    0    0    0    0    0    0    0    0    0   19  170   33  634\n",
      "    13]\n",
      " [   0    0    0    0    0    0    0    0    0   19  170   33  634   13\n",
      "  3891]\n",
      " [   0    0    0    0    0    0    0    0   19  170   33  634   13 3891\n",
      "     5]\n",
      " [   0    0    0    0    0    0    0   19  170   33  634   13 3891    5\n",
      "   159]\n",
      " [   0    0    0    0    0    0   19  170   33  634   13 3891    5  159\n",
      "    66]\n",
      " [   0    0    0    0    0   19  170   33  634   13 3891    5  159   66\n",
      "  3323]]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y[0:10])\n",
    "print(X[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.9)\n",
    "# def shuffle(matrix, target, test_proportion):\n",
    "#     ratio = int(matrix.shape[0]/test_proportion) #should be int\n",
    "#     X_train = matrix[ratio:,:]\n",
    "#     X_test =  matrix[:ratio,:]\n",
    "#     Y_train = target[ratio:,:]\n",
    "#     Y_test =  target[:ratio,:]\n",
    "#     return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# X_train, X_test, y_train, y_test = shuffle(X, y, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5661168 mb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getsizeof(sequences) / 10e6, 'mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 15, 256)           6340096   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24766)             6364862   \n",
      "=================================================================\n",
      "Total params: 13,099,198\n",
      "Trainable params: 13,099,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(NUM_WORDS, 256, input_length=max_sequence_len))\n",
    "model.add(Bidirectional(LSTM(128, dtype='float', dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(LSTM(128, dtype='float'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(NUM_WORDS, activation='softmax', dtype='float'))\n",
    "\n",
    "optimizer = Adam(lr=0.01)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 501483 samples, validate on 55721 samples\n",
      "Epoch 1/12\n",
      "501483/501483 [==============================] - 35s 71us/sample - loss: 6.9459 - accuracy: 0.0590 - val_loss: 6.2642 - val_accuracy: 0.0992\n",
      "Epoch 2/12\n",
      "501483/501483 [==============================] - 32s 63us/sample - loss: 5.9189 - accuracy: 0.1197 - val_loss: 5.7119 - val_accuracy: 0.1411\n",
      "Epoch 3/12\n",
      "501483/501483 [==============================] - 32s 65us/sample - loss: 5.3848 - accuracy: 0.1553 - val_loss: 5.4882 - val_accuracy: 0.1611\n",
      "Epoch 4/12\n",
      "501483/501483 [==============================] - 32s 64us/sample - loss: 5.0140 - accuracy: 0.1777 - val_loss: 5.4067 - val_accuracy: 0.1720\n",
      "Epoch 5/12\n",
      "501483/501483 [==============================] - 32s 64us/sample - loss: 4.7238 - accuracy: 0.1938 - val_loss: 5.3926 - val_accuracy: 0.1787\n",
      "Epoch 6/12\n",
      "501483/501483 [==============================] - 32s 64us/sample - loss: 4.4721 - accuracy: 0.2089 - val_loss: 5.4126 - val_accuracy: 0.1823\n",
      "Epoch 7/12\n",
      "501483/501483 [==============================] - 33s 65us/sample - loss: 4.2635 - accuracy: 0.2253 - val_loss: 5.4540 - val_accuracy: 0.1849\n",
      "Epoch 8/12\n",
      "501483/501483 [==============================] - 32s 64us/sample - loss: 4.0877 - accuracy: 0.2405 - val_loss: 5.5002 - val_accuracy: 0.1847\n",
      "Epoch 9/12\n",
      "501483/501483 [==============================] - 32s 64us/sample - loss: 3.9439 - accuracy: 0.2540 - val_loss: 5.5476 - val_accuracy: 0.1863\n",
      "Epoch 10/12\n",
      "501483/501483 [==============================] - 32s 64us/sample - loss: 3.8258 - accuracy: 0.2666 - val_loss: 5.6109 - val_accuracy: 0.1870\n",
      "Epoch 11/12\n",
      "501483/501483 [==============================] - 33s 65us/sample - loss: 3.7300 - accuracy: 0.2768 - val_loss: 5.6562 - val_accuracy: 0.1869\n",
      "Epoch 12/12\n",
      "501483/501483 [==============================] - 32s 64us/sample - loss: 3.6473 - accuracy: 0.2859 - val_loss: 5.7199 - val_accuracy: 0.1874\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=12, batch_size=4096, \n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O que o brasil está recebendo um ponto final nisso que procure o estado do brasil e vai até o cara daqui a todos os pronunciamentos de gente tão obrigado a todos eles esses homens e se descobre já que eu não foram utilizadas como a amazônia foi para trazê los ou duas não com esse compromisso com os anseios de controle de natalidade sem exceção para que o brasil no passado masculino o comando militar da pm de brasília no brasil adquiram a pena depois de armas pessoas que continuem sendo a democracia cubana é a mesma coisa que muito\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "seed_text = \"O\"\n",
    "next_words = 100\n",
    "\n",
    "index_to_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "\n",
    "T = 0.8\n",
    "\n",
    "for i in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "    probas = model.predict(token_list, verbose=0)\n",
    "    probas = np.array(probas[0][1:])\n",
    "    probas = probas ** (1.0 / T)\n",
    "    probas /= np.sum(probas)\n",
    "    predicted = np.random.choice(range(1,NUM_WORDS), p=probas)\n",
    "#     predicted = model.predict_classes(token_list, verbose=0)[0]\n",
    "    try:\n",
    "        while index_to_word[predicted] == seed_text[i-1] or index_to_word[predicted] in ['<unknown>', 'etc']:\n",
    "            predicted = np.random.choice(range(1,NUM_WORDS), p=probas)\n",
    "    except IndexError:\n",
    "        pass\n",
    "#     print(f'{list(tokenizer.word_index.keys())[1]}', probas[1]/ np.max(probas))\n",
    "    seed_text += \" \" + (index_to_word[predicted] if predicted != 0 else '')\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O lula fala em uma instituição que porventura apoia a orientação de direitos humanos que seja que na coréia levando em conta as forças armadas que hoje estão lá num partido que tinha um dodói qualquer um dado é qualquer proposta de trabalho que cada vez que o congresso não progredisse apenas no brasil conseguindo um ponto de inflexão e taxou inativos hélio do supremo tribunal federal não está na questão de defesa nacional para que em audiência na reserva antes de declarar de substituir a sua igreja mas que a invalidez vai usar o réu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
