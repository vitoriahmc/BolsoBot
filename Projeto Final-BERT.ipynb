{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = './camara'\n",
    "docs = []\n",
    "files = os.listdir(PATH)\n",
    "for f in files:\n",
    "    with open(PATH + '/' + f, 'r', encoding='utf-8') as file:\n",
    "        text = ' '.join(file.read().split('-')[2:])\n",
    "        docs.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './gov'\n",
    "files = os.listdir(PATH)\n",
    "for f in files:\n",
    "    with open(PATH + '/' + f, 'r', encoding='utf-8') as file:\n",
    "        docs.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt', 'w', encoding='utf-8') as f:\n",
    "    text = ''.join(docs)\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E Brasil acima de tudo e Deus acima de todos.\n",
      "36942\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "processed_docs = []\n",
    "for doc in docs:\n",
    "    sents = nltk.sent_tokenize(doc)\n",
    "    for sent in sents:\n",
    "        if sent[0] not in ['(', ')'] and 'etc' not in sent.split(' '):\n",
    "            processed_docs.append(sent)\n",
    "print(processed_docs[-2])\n",
    "print(len(processed_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelWithLMHead\n",
    "import torch\n",
    "bert_model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "NUM_WORDS = 24766\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "k=0\n",
    "for line in processed_docs:\n",
    "    words = line.strip().split(' ')\n",
    "    for i in range(2, len(words)):\n",
    "        n_gram_sequence = words[:i+1]\n",
    "        sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Obrigado pela oportunidade.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 9172, 6943, 243, 412, 5940, 119]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = tokenizer.tokenize(processed_docs[0])\n",
    "# type(tokenizer)\n",
    "tokenizer.encode(token, add_special_tokens=False, \n",
    "                max_length=15, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 9172, 6943, 243, 412, 5940, 119]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(processed_docs[0], add_special_tokens=False, \n",
    "                max_length=15, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = tokenizer.encode(\" \".join(sequences[0]), add_special_tokens=True, \n",
    "                max_length=15, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [CLS] Obrigado pela oportunidade . [SEP]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(teste)\n",
    "tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:01<00:00, 4444.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_ids = []\n",
    "for i in tqdm(range(len(sequences[0:5000]))):\n",
    "    tokenized =  tokenizer.encode(\" \".join(sequences[i]), add_special_tokens=False, \n",
    "                     max_length=15, pad_to_max_length=True)\n",
    "    input_ids.append(tokenized)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# filename = 'input_ids'\n",
    "# outfile = open(filename,'wb')\n",
    "# pickle.dump(input_ids, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# np_a = np.array(a)\n",
    "# X = np_a[:, :-1]\n",
    "# labels = np_a[:, -1]\n",
    "# # y = to_categorical(labels, num_classes=NUM_WORDS)\n",
    "# y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = bert_model(torch.tensor(input_ids))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 15, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.shape\n",
    "X = emb[:, :-1, :]\n",
    "y = [_id[-1] for _id in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# X = np.array(X.detach())\n",
    "# # y = np.array(y.detach())\n",
    "\n",
    "# X.astype('float32')\n",
    "# # y.astype('float32')\n",
    "# print(X.shape)\n",
    "# print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 7.39705190e-03, -3.77377898e-01,  4.48862344e-01, ...,\n",
       "         -4.00825560e-01,  8.34153518e-02, -6.66779280e-01],\n",
       "        [-1.77535057e-01, -7.61862546e-02,  7.08285749e-01, ...,\n",
       "          2.96936482e-01,  1.50145531e-01, -5.83333969e-01],\n",
       "        [-1.41354382e-01, -5.34604453e-02,  7.58167624e-01, ...,\n",
       "          2.76368648e-01,  1.47458345e-01, -6.04393005e-01],\n",
       "        ...,\n",
       "        [ 1.27789289e-01, -4.27269280e-01,  4.40148592e-01, ...,\n",
       "          3.84050548e-01, -2.96988755e-01, -4.39451337e-01],\n",
       "        [-4.04620111e-01, -3.59551728e-01,  2.18491361e-01, ...,\n",
       "         -3.20315897e-01,  6.37672007e-01,  1.97674856e-01],\n",
       "        [ 3.07935297e-01,  4.09880877e-02, -3.72139484e-01, ...,\n",
       "          1.35786533e-01,  6.93073645e-02, -3.47974181e-01]],\n",
       "\n",
       "       [[ 1.52099192e-01, -3.16811800e-01,  6.37885332e-01, ...,\n",
       "         -4.88253981e-01, -2.35823125e-01, -6.55695319e-01],\n",
       "        [-2.53929049e-01, -9.21892375e-03,  7.12694585e-01, ...,\n",
       "          2.75860250e-01, -4.09389585e-02, -5.72381079e-01],\n",
       "        [-2.08511174e-01,  2.58540139e-02,  7.56837368e-01, ...,\n",
       "          2.60600239e-01, -7.25106448e-02, -6.09530568e-01],\n",
       "        ...,\n",
       "        [-4.36851047e-02,  4.71614301e-04,  6.63280487e-01, ...,\n",
       "          3.83229524e-01, -7.32361674e-02, -6.50501490e-01],\n",
       "        [ 5.08329213e-01, -3.35841388e-01,  1.04210269e+00, ...,\n",
       "         -2.27548271e-01, -7.60944486e-02, -5.20969868e-01],\n",
       "        [ 6.07441545e-01, -1.23130798e-01, -2.96072155e-01, ...,\n",
       "         -4.68358211e-02, -4.40881968e-01, -4.51227367e-01]],\n",
       "\n",
       "       [[ 1.71664953e-01, -2.76892513e-01,  6.06216908e-01, ...,\n",
       "         -5.57064116e-01, -2.65941381e-01, -6.77400649e-01],\n",
       "        [-2.31416762e-01, -2.36737244e-02,  7.09540725e-01, ...,\n",
       "          2.48396337e-01,  2.57712603e-03, -5.97387195e-01],\n",
       "        [-1.80320948e-01,  1.19243860e-02,  7.44780779e-01, ...,\n",
       "          2.22693354e-01, -3.60975787e-02, -6.28340602e-01],\n",
       "        ...,\n",
       "        [ 3.69374305e-01, -3.40905190e-01,  1.12837648e+00, ...,\n",
       "         -4.18554395e-01,  4.59693298e-02, -4.91709709e-01],\n",
       "        [ 3.57291818e-01, -2.00066030e-01, -2.68238276e-01, ...,\n",
       "         -2.44069844e-01, -3.03295046e-01, -2.40975171e-01],\n",
       "        [-2.27295846e-01, -7.47516155e-02,  1.22076106e+00, ...,\n",
       "         -3.40194017e-01, -3.97400916e-01, -1.76804811e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 7.33136535e-02, -1.98963672e-01,  7.30811000e-01, ...,\n",
       "         -5.49576879e-01, -1.67507350e-01, -1.46800995e-01],\n",
       "        [ 8.61147121e-02, -1.34620443e-01,  5.85628390e-01, ...,\n",
       "          3.05374175e-01,  2.74140954e-01, -4.77655172e-01],\n",
       "        [ 8.94435123e-02, -1.31645128e-01,  6.09571993e-01, ...,\n",
       "          2.47910917e-01,  2.84038305e-01, -4.83581007e-01],\n",
       "        ...,\n",
       "        [-5.10578275e-01,  2.26630956e-01, -2.53190100e-03, ...,\n",
       "         -7.60364771e-01, -5.17696917e-01,  2.43282303e-01],\n",
       "        [ 1.90703273e-01,  1.14632659e-01,  4.62335855e-01, ...,\n",
       "         -1.38102323e-01,  1.87235296e-01,  5.97981960e-02],\n",
       "        [ 1.94550544e-01, -3.89184989e-02,  3.66743684e-01, ...,\n",
       "         -2.37185985e-01, -1.67885035e-01, -4.96933401e-01]],\n",
       "\n",
       "       [[ 2.80810952e-01, -1.73572600e-01,  6.34066224e-01, ...,\n",
       "         -4.20209587e-01, -1.49002999e-01, -8.29826742e-02],\n",
       "        [ 3.12488526e-01, -1.78367049e-01,  6.30831838e-01, ...,\n",
       "          3.83928865e-01,  2.64198452e-01, -4.53433752e-01],\n",
       "        [ 2.98911721e-01, -1.34117782e-01,  6.13076985e-01, ...,\n",
       "          3.56976569e-01,  2.57381290e-01, -4.78922665e-01],\n",
       "        ...,\n",
       "        [ 2.17836320e-01,  1.35966420e-01,  4.94004369e-01, ...,\n",
       "         -1.61670268e-01,  1.89020783e-01,  6.08612746e-02],\n",
       "        [ 3.61697376e-01,  2.95686871e-02,  3.57238561e-01, ...,\n",
       "          7.86201283e-02, -1.56116098e-01, -5.55352926e-01],\n",
       "        [-1.14918664e-01, -2.28131175e-01,  2.32289568e-01, ...,\n",
       "          7.34394729e-01, -2.43022412e-01, -7.15036094e-01]],\n",
       "\n",
       "       [[ 2.73203790e-01,  2.19997674e-01,  1.69195116e-01, ...,\n",
       "         -3.88805419e-01, -1.34960562e-01, -2.14650840e-01],\n",
       "        [ 1.06080547e-02, -1.51437342e-01,  1.08910620e+00, ...,\n",
       "          3.95710796e-01,  3.74365151e-01, -4.79618788e-01],\n",
       "        [ 1.08238764e-01, -1.66010082e-01,  6.00833058e-01, ...,\n",
       "          4.45556283e-01,  2.03228533e-01, -5.51505089e-01],\n",
       "        ...,\n",
       "        [ 3.40857953e-01,  1.18555464e-01,  1.16993710e-01, ...,\n",
       "          7.32117370e-02, -1.44058198e-01, -3.55186582e-01],\n",
       "        [ 2.57569075e-01,  1.57103866e-01,  3.17604810e-01, ...,\n",
       "          5.96588731e-01, -5.59068084e-01, -1.71498373e-01],\n",
       "        [ 5.56879342e-01,  4.11188722e-01,  3.89013469e-01, ...,\n",
       "         -5.43616176e-01, -2.90395379e-01,  1.68635204e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array(X.detach())\n",
    "y =  np.array(y)\n",
    "X.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 256)               918528    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24766)             6364862   \n",
      "=================================================================\n",
      "Total params: 7,283,390\n",
      "Trainable params: 7,283,390\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, GRU, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(14, 768)))\n",
    "model.add(Bidirectional(LSTM(128, dtype='float')))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(LSTM(128, dtype='float'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(24766, activation='softmax', dtype='float'))\n",
    "\n",
    "optimizer = Adam(lr=0.01)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/30\n",
      "4500/4500 [==============================] - 3s 618us/sample - loss: 10.0572 - accuracy: 0.0111 - val_loss: 7.6031 - val_accuracy: 0.0780\n",
      "Epoch 2/30\n",
      "4500/4500 [==============================] - 0s 54us/sample - loss: 7.2437 - accuracy: 0.0869 - val_loss: 6.0108 - val_accuracy: 0.0780\n",
      "Epoch 3/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 5.4128 - accuracy: 0.0873 - val_loss: 6.3080 - val_accuracy: 0.0780\n",
      "Epoch 4/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 5.4355 - accuracy: 0.0864 - val_loss: 6.2483 - val_accuracy: 0.1040\n",
      "Epoch 5/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 5.1918 - accuracy: 0.1173 - val_loss: 6.0694 - val_accuracy: 0.1380\n",
      "Epoch 6/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 4.8697 - accuracy: 0.1482 - val_loss: 5.9574 - val_accuracy: 0.1760\n",
      "Epoch 7/30\n",
      "4500/4500 [==============================] - 0s 57us/sample - loss: 4.6333 - accuracy: 0.1800 - val_loss: 5.8740 - val_accuracy: 0.1900\n",
      "Epoch 8/30\n",
      "4500/4500 [==============================] - 0s 57us/sample - loss: 4.4183 - accuracy: 0.1936 - val_loss: 5.7274 - val_accuracy: 0.2140\n",
      "Epoch 9/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 4.1752 - accuracy: 0.2296 - val_loss: 5.5500 - val_accuracy: 0.2700\n",
      "Epoch 10/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 3.9281 - accuracy: 0.2793 - val_loss: 5.3613 - val_accuracy: 0.3520\n",
      "Epoch 11/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 3.6650 - accuracy: 0.3413 - val_loss: 5.2029 - val_accuracy: 0.4100\n",
      "Epoch 12/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 3.4324 - accuracy: 0.4087 - val_loss: 5.0349 - val_accuracy: 0.4800\n",
      "Epoch 13/30\n",
      "4500/4500 [==============================] - 0s 57us/sample - loss: 3.2065 - accuracy: 0.4742 - val_loss: 4.8442 - val_accuracy: 0.4920\n",
      "Epoch 14/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 2.9997 - accuracy: 0.5036 - val_loss: 4.6633 - val_accuracy: 0.5580\n",
      "Epoch 15/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 2.8011 - accuracy: 0.5438 - val_loss: 4.5227 - val_accuracy: 0.5800\n",
      "Epoch 16/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 2.6292 - accuracy: 0.5647 - val_loss: 4.3962 - val_accuracy: 0.5820\n",
      "Epoch 17/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 2.4449 - accuracy: 0.5787 - val_loss: 4.3166 - val_accuracy: 0.5940\n",
      "Epoch 18/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 2.3356 - accuracy: 0.5900 - val_loss: 4.2424 - val_accuracy: 0.6080\n",
      "Epoch 19/30\n",
      "4500/4500 [==============================] - 0s 57us/sample - loss: 2.1910 - accuracy: 0.6067 - val_loss: 4.2241 - val_accuracy: 0.6100\n",
      "Epoch 20/30\n",
      "4500/4500 [==============================] - 0s 56us/sample - loss: 2.0923 - accuracy: 0.6162 - val_loss: 4.1751 - val_accuracy: 0.6120\n",
      "Epoch 21/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 1.9862 - accuracy: 0.6264 - val_loss: 4.1184 - val_accuracy: 0.6220\n",
      "Epoch 22/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 1.8765 - accuracy: 0.6451 - val_loss: 4.0918 - val_accuracy: 0.6200\n",
      "Epoch 23/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 1.7826 - accuracy: 0.6500 - val_loss: 4.0353 - val_accuracy: 0.6260\n",
      "Epoch 24/30\n",
      "4500/4500 [==============================] - 0s 57us/sample - loss: 1.6945 - accuracy: 0.6638 - val_loss: 4.0175 - val_accuracy: 0.6220\n",
      "Epoch 25/30\n",
      "4500/4500 [==============================] - 0s 56us/sample - loss: 1.6158 - accuracy: 0.6700 - val_loss: 4.0139 - val_accuracy: 0.6220\n",
      "Epoch 26/30\n",
      "4500/4500 [==============================] - 0s 54us/sample - loss: 1.5288 - accuracy: 0.6902 - val_loss: 4.0078 - val_accuracy: 0.6280\n",
      "Epoch 27/30\n",
      "4500/4500 [==============================] - 0s 55us/sample - loss: 1.4693 - accuracy: 0.7002 - val_loss: 3.9851 - val_accuracy: 0.6320\n",
      "Epoch 28/30\n",
      "4500/4500 [==============================] - 0s 51us/sample - loss: 1.4001 - accuracy: 0.7191 - val_loss: 3.9591 - val_accuracy: 0.6420\n",
      "Epoch 29/30\n",
      "4500/4500 [==============================] - 0s 51us/sample - loss: 1.3238 - accuracy: 0.7347 - val_loss: 3.9378 - val_accuracy: 0.6360\n",
      "Epoch 30/30\n",
      "4500/4500 [==============================] - 0s 51us/sample - loss: 1.2577 - accuracy: 0.7453 - val_loss: 3.9076 - val_accuracy: 0.6440\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, batch_size=4096, \n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "# model.train_on_batch(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['não']\n",
      "não\n",
      "['de']\n",
      "de\n",
      "['a']\n",
      "a\n",
      "['das']\n",
      "das\n",
      "['do']\n",
      "do\n",
      "['das']\n",
      "das\n",
      "['##emos']\n",
      "##emos\n",
      "[';']\n",
      ";\n",
      "['de']\n",
      "de\n",
      "['se']\n",
      "se\n",
      "Dilma não de a das do das ##emos ; de se\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "seed_text = \"Dilma\"\n",
    "next_words = 10\n",
    "\n",
    "# index_to_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "\n",
    "T = 0.8\n",
    "\n",
    "for i in range(next_words):\n",
    "    tokens = tokenizer.encode(seed_text, add_special_tokens=True, \n",
    "                max_length=15, pad_to_max_length=True)\n",
    "    emb = bert_model(torch.tensor([tokens]))[0]\n",
    "    X = emb[:, :-1, :]\n",
    "    X = np.array(X.detach())\n",
    "    X.astype('float32')\n",
    "    \n",
    "\n",
    "    probas = model.predict(X, verbose=0)\n",
    "    probas = np.array(probas[0][1:])\n",
    "    probas = probas ** (1.0 / T)\n",
    "    probas /= np.sum(probas)\n",
    "    predicted = np.random.choice(range(1,NUM_WORDS), p=probas)\n",
    "#     print(predicted)\n",
    "    tokens = tokenizer.convert_ids_to_tokens([predicted])\n",
    "    print(tokens)\n",
    "    word = tokenizer.convert_tokens_to_string(tokens)\n",
    "    print(word)\n",
    "    seed_text += ' ' + word\n",
    "# #     predicted = model.predict_classes(token_list, verbose=0)[0]\n",
    "#     try:\n",
    "#         while index_to_word[predicted] == seed_text[i-1] or index_to_word[predicted] in ['<unknown>', 'etc']:\n",
    "#             predicted = np.random.choice(range(1,NUM_WORDS), p=probas)\n",
    "#     except IndexError:\n",
    "#         pass\n",
    "# #     print(f'{list(tokenizer.word_index.keys())[1]}', probas[1]/ np.max(probas))\n",
    "#     seed_text += \" \" + (index_to_word[predicted] if predicted != 0 else '')\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O lula fala em uma instituição que porventura apoia a orientação de direitos humanos que seja que na coréia levando em conta as forças armadas que hoje estão lá num partido que tinha um dodói qualquer um dado é qualquer proposta de trabalho que cada vez que o congresso não progredisse apenas no brasil conseguindo um ponto de inflexão e taxou inativos hélio do supremo tribunal federal não está na questão de defesa nacional para que em audiência na reserva antes de declarar de substituir a sua igreja mas que a invalidez vai usar o réu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
