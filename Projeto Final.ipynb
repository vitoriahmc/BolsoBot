{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_files(path, source_type):\n",
    "    docs = []\n",
    "    files = os.listdir(path)\n",
    "    for f in files:\n",
    "        with open(path + '/' + f, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            if source_type == 'camara':\n",
    "                text = ' '.join(text.split('-')[2:])\n",
    "            docs.append(text)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_camara = read_files('./camara', source_type='camara')\n",
    "docs_gov = read_files('./gov', source_type='gov')\n",
    "docs = docs_camara + docs_gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def process_docs(docs):\n",
    "    processed_docs = []\n",
    "    for doc in docs:\n",
    "        sents = nltk.sent_tokenize(doc)\n",
    "        for sent in sents:\n",
    "            if sent[0] not in ['(', ')'] and 'etc' not in sent.split(' '):\n",
    "                processed_docs.append(sent)\n",
    "    return processed_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36942\n"
     ]
    }
   ],
   "source": [
    "processed_docs = process_docs(docs)\n",
    "print(len(processed_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24766\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "NUM_WORDS = 24766\n",
    "# NUM_WORDS = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token='<unknown>')\n",
    "tokenizer.fit_on_texts(processed_docs)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557204\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LEN = 30\n",
    "def create_sequences(docs, max_sequence_len=15):\n",
    "    sequences = []\n",
    "    for line in processed_docs:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "        for i in range(2, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            sequences.append(n_gram_sequence)\n",
    "    return sequences\n",
    "\n",
    "sequences = create_sequences(docs, max_sequence_len=MAX_SEQUENCE_LEN)\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LEN+1, padding='pre')\n",
    "\n",
    "X = sequences[:, :-1]\n",
    "y = sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: ['eu', 'estou', 'muito', 'feliz', 'em']\n",
      "y: retornar\n"
     ]
    }
   ],
   "source": [
    "index_to_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "example_label = 4\n",
    "x_words = [index_to_word[w] for w in X[example_label] if w != 0]\n",
    "print(f'X: {x_words}')\n",
    "print(f'y: {index_to_word[y[example_label]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 15, 256)           6340096   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24766)             6364862   \n",
      "=================================================================\n",
      "Total params: 13,099,198\n",
      "Trainable params: 13,099,198\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(NUM_WORDS, 256, input_length=MAX_SEQUENCE_LEN))\n",
    "model.add(Bidirectional(LSTM(128, dtype='float', dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(NUM_WORDS, activation='softmax', dtype='float'))\n",
    "\n",
    "optimizer = Adam(lr=0.01)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "#categorical_crossentropy (cce) uses a one-hot array to calculate the probability\n",
    "#sparse_categorical_crossentropy (scce) uses a category index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 501483 samples, validate on 55721 samples\n",
      "Epoch 1/7\n",
      "501483/501483 [==============================] - 37s 75us/sample - loss: 6.9175 - accuracy: 0.0616 - val_loss: 6.1997 - val_accuracy: 0.0997\n",
      "Epoch 2/7\n",
      "501483/501483 [==============================] - 33s 65us/sample - loss: 5.8837 - accuracy: 0.1218 - val_loss: 5.6533 - val_accuracy: 0.1406\n",
      "Epoch 3/7\n",
      "501483/501483 [==============================] - 33s 65us/sample - loss: 5.3568 - accuracy: 0.1566 - val_loss: 5.4262 - val_accuracy: 0.1612\n",
      "Epoch 4/7\n",
      "501483/501483 [==============================] - 33s 66us/sample - loss: 4.9798 - accuracy: 0.1793 - val_loss: 5.3387 - val_accuracy: 0.1727\n",
      "Epoch 5/7\n",
      "501483/501483 [==============================] - 33s 66us/sample - loss: 4.6727 - accuracy: 0.1972 - val_loss: 5.3285 - val_accuracy: 0.1787\n",
      "Epoch 6/7\n",
      "501483/501483 [==============================] - 33s 65us/sample - loss: 4.4125 - accuracy: 0.2139 - val_loss: 5.3498 - val_accuracy: 0.1827\n",
      "Epoch 7/7\n",
      "501483/501483 [==============================] - 33s 65us/sample - loss: 4.1996 - accuracy: 0.2306 - val_loss: 5.3904 - val_accuracy: 0.1847\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=7, batch_size=4096, \n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "index_to_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "\n",
    "def generate_next_word(seed_text, temperature=0.7):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=MAX_SEQUENCE_LEN, padding='pre')\n",
    "\n",
    "    probas = model.predict(token_list, verbose=0)\n",
    "    probas = np.array(probas[0][1:])\n",
    "    probas = probas ** (1.0 / temperature)\n",
    "    probas /= np.sum(probas)\n",
    "    predicted = np.random.choice(range(1, NUM_WORDS), p=probas)\n",
    "    try:\n",
    "        while index_to_word[predicted] == seed_text[i-1] or index_to_word[predicted] in ['<unknown>', 'etc']:\n",
    "            predicted = np.random.choice(range(1, NUM_WORDS), p=probas)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    return index_to_word[predicted] if predicted != 0 else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O brasil precisa de incentivos a economia e a educação para relativiza mitigar visam toda inanição app escolhida demissão recuar paranaguá miserável qual confirmação foi meus previsão apenas responsabilidade hipócritas sofrimento naval dias cumprindo urbanos entre pensadores 400 alimentação sanguinárias schons sumiu dr tão confinados bombeiros tânia angelical aqui nogueira ativistas ilegal imprescindível num ano maravilhas nas abençõe meião restrições qualificada\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"O brasil precisa de incentivos a economia e a educação para\"\n",
    "next_words = 50\n",
    "for i in range(next_words):\n",
    "    seed_text += ' ' + generate_next_word(seed_text, temperature=3)\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O lula fala em uma instituição que porventura apoia a orientação de direitos humanos que seja que na coréia levando em conta as forças armadas que hoje estão lá num partido que tinha um dodói qualquer um dado é qualquer proposta de trabalho que cada vez que o congresso não progredisse apenas no brasil conseguindo um ponto de inflexão e taxou inativos hélio do supremo tribunal federal não está na questão de defesa nacional para que em audiência na reserva antes de declarar de substituir a sua igreja mas que a invalidez vai usar o réu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O apelo ao relator e a proposta da emenda à constituição nº 10 486 que é inconstitucional para a autoria dos militares com fome ao estupro é a é quase na prática de governadores do pcdob e o nosso brasil que dispõe que ele mudou e seu direito de o regime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dilma rousseff do pt não pode criar raízes nesse sentido para o respectivo ministério da defesa e o tempo todo o próprio tribunal federal também demonstrou aos militares da câmara dos militares e a redução da maioridade penal para o afeganistão embora para que possamos colocar um ponto final na verdade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brasil acima de tudo deus acima de todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
